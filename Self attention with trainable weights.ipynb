{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c7a579-a074-4d3c-9849-3e7bce27677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46e42981-129d-493e-887a-d2f6f8ecc709",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "     [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one\n",
    "     [0.05, 0.80, 0.55]  # step\n",
    "     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b29043-1e56-4286-908e-838555596fc6",
   "metadata": {},
   "source": [
    "- VARABLES\n",
    "- #A The second input element\n",
    "- #B The input embedding size, d=3\n",
    "- #C The output embedding size, d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd9789e5-cf1a-4fe6-b72a-ac8fea8197f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In gpt like models the input and output dimensions are usually the same \n",
    "# Here we choose different input (d_in = 3) and output (d_out=2) \n",
    "\n",
    "x_2 = inputs[1] #A --> word journey\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9397ae29-540e-489b-8773-e1694eb1be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize three weight matrices Wq, Wk, Wv\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad= False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad= False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c4d502d-263b-4f5c-8a3d-6791ca76ed88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n"
     ]
    }
   ],
   "source": [
    "print(W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b2c411b-ea1b-4dd4-95a6-06445802dc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n"
     ]
    }
   ],
   "source": [
    "print(W_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fad77b2e-65af-4616-8bf3-e8cae61be502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "print(W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b35bc232-4b5c-4980-91a0-08030341e6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# We are just working for the word journey\n",
    "\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ea95995-cfb6-4262-8d98-e2d1b4759eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "queries.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "queries = inputs @ W_query\n",
    "\n",
    "print(\"key.shape:\", keys.shape)\n",
    "\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "print(\"queries.shape:\", queries.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79e801-4391-4948-9ac1-b83fe86f6859",
   "metadata": {},
   "source": [
    "- We have successfully projected the 6 input tokens from a 3D onto a 2D embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f4eaa1f-2234-48d6-94ac-68729d1dc04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# Attention score w22\n",
    "\n",
    "keys_2 = keys[1] #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75686ad6-dcb7-4670-906e-316b6f33fcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# We can generalize this computation to all attention scores via matrix multiplication\n",
    "attn_score_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_score_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9d02f11-d879-4143-ae1b-a4fba720d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = queries @ keys.T # omega\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976f1a2-21ce-4132-99e0-112ee0c90f66",
   "metadata": {},
   "source": [
    "- We compute the attention weights by scaling the attention scores and using the softmax function\n",
    "\n",
    "- The difference to earlier is that we now scale the attention scores by dividing them by the square root of keu dimensions\n",
    "\n",
    "- Taking square root is mathematically the same as exponentiating by 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a5d9906-bddc-42a6-a1cb-969072bf6a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1] # keys dimension - (-1) as we are looking at the column\n",
    "attn_weights_2 = torch.softmax(attn_score_2 / d_k ** 0.5, dim=-1)\n",
    "print(attn_weights_2)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3cf1bd-5f0b-472f-b6bb-dbd3170007c7",
   "metadata": {},
   "source": [
    "Why divide by sqrt (dimensions)\n",
    "\n",
    "- For stability in learning : The softmax function is sensitive to the magnitudes of the inputs. When the inputs are large, the differences between the exponential values of each input become much more pronounced. This causes the softmax output to become \"peaky\" where the highest value receives almost all the probability mass, and the rest receive very litte.\n",
    "- In attention mechanisms, particularly in tranformers, if the dot products between query and key vectors become too large, the attention scores can become very large. This results in a very sharp softmax distribution, making the model overly confident in one particular \"key\". Such sharp distributions can make learnings unstable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abfb179e-2b08-44af-a60c-e6b1821a8518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax without scaling: tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "Softmax after scaling (tensor * 8): tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Define the tensor\n",
    "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "\n",
    "# Apply softmax without scaling \n",
    "softmax_result = torch.softmax(tensor, dim= -1)\n",
    "print(\"Softmax without scaling:\", softmax_result)\n",
    "\n",
    "# Multiply the tensor by 8 and then apply softmax \n",
    "scaled_tensor = tensor * 8\n",
    "softmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\n",
    "print(\"Softmax after scaling (tensor * 8):\", softmax_scaled_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c09111-57f2-4233-8f10-11be0d8465fd",
   "metadata": {},
   "source": [
    "- Softmax of first tensor are good, but for the second tensor they are disproportionately high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d944f814-120e-4b07-b441-f28d0ae5f5cd",
   "metadata": {},
   "source": [
    "Why sqrt ? \n",
    "- To make the variance of the dot product stable\n",
    "- The dot product of Q and K increases the variance because multiplying two random numbers increases the variance\n",
    "- The increase in variance grows with the dimension\n",
    "- Dividing by sqrt (dimension) keeps the variance close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5f4362f-cd07-4b08-aeb7-a98cbab290f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance before scaling (dim=5): 4.987516464377303\n",
      "Variance after scaling (dim=5): 0.9975032928754606\n",
      "Variance before scaling (dim=20): 18.447448718368197\n",
      "Variance after scaling (dim=20): 0.9223724359184097\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compute variance before and after scaling\n",
    "def compute_variance(dim, num_trails=1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute dot products \n",
    "    for _ in range(num_trails):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "\n",
    "        # Compute dot product\n",
    "        dot_product = np.dot (q,k)\n",
    "        dot_products.append(dot_product)\n",
    "\n",
    "        # Scale the dot product by sqrt(dim)\n",
    "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "\n",
    "    # Calculate variance of the dot products \n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "# For dimension 5\n",
    "variance_before_5, variance_after_5 = compute_variance(5)\n",
    "print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n",
    "print(f\"Variance after scaling (dim=5): {variance_after_5}\")\n",
    "\n",
    "# For dimension 20\n",
    "variance_before_20, variance_after_20 = compute_variance(20)\n",
    "print(f\"Variance before scaling (dim=20): {variance_before_20}\")\n",
    "print(f\"Variance after scaling (dim=20): {variance_after_20}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377ab9c-a0f9-4ff2-8e9f-184ae349f033",
   "metadata": {},
   "source": [
    "- If dimension = 5, then the variance is close to 5\n",
    "- After scaling the variance is always close to 1 --> that's why sqrt is used "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a79b5-c3b1-445b-aa43-bf8e55d795c1",
   "metadata": {},
   "source": [
    "- Context vector is a weighted sum over the value vectors \n",
    "- Here, the attention weights serve as a weighting factor that weighs the respective importance of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6bf04f3c-56cc-434c-aa22-c7748adab944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19069037-d2e2-4ee6-8eb4-cbe54c7266f1",
   "metadata": {},
   "source": [
    "# Implementing a compact self attention python class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b266045-5864-4862-879d-a74fbaed5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out): \n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context_vec = attn_weights @ values \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e9be9-8652-4727-92d2-2bed58e9e1d4",
   "metadata": {},
   "source": [
    "- In PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a fundamental building block of PyTorch models, which provides necessary functionalities for model layer creation and management\n",
    "- The init method initializes trainable weight matrices (W_query, W_key and W_value) for queries, keys and values, each transforming the input dimension d_in to an output dimension d_out.\n",
    "- During the forward pass, using the forward method, we compute the attention scores (attn_scores) by multiplying queries and keys, normalizing these scores using softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8af52b42-a609-486e-9992-8d0c2c53efe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5156f78f-8729-4b6f-af77-bdce0be00c41",
   "metadata": {},
   "source": [
    "- We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's nn.Linear layers, which effectively perform matrix multiplication when the bias units are diabled.\n",
    "- Additionally, a significant advantage of using nn.Linear instead of manually\n",
    "implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight\n",
    "initialization scheme, contributing to more stable and effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d2245c7-db92-4b82-aa1a-493e30f6eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False): \n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context_vec = attn_weights @ values \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "836aff4d-980d-4bf9-9a3d-0efc0d3a32eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415f296-4602-4478-bb1e-dbd4fef8abe8",
   "metadata": {},
   "source": [
    "- Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they\n",
    "use different initial weights for the weight matrices since nn.Linear uses a more\n",
    "sophisticated weight initialization scheme."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
