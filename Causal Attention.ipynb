{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c7a579-a074-4d3c-9849-3e7bce27677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e42981-129d-493e-887a-d2f6f8ecc709",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "     [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one\n",
    "     [0.05, 0.80, 0.55]  # step\n",
    "     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b29043-1e56-4286-908e-838555596fc6",
   "metadata": {},
   "source": [
    "- VARABLES\n",
    "- #A The second input element\n",
    "- #B The input embedding size, d=3\n",
    "- #C The output embedding size, d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9789e5-cf1a-4fe6-b72a-ac8fea8197f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In gpt like models the input and output dimensions are usually the same \n",
    "# Here we choose different input (d_in = 3) and output (d_out=2) \n",
    "\n",
    "x_2 = inputs[1] #A --> word journey\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9397ae29-540e-489b-8773-e1694eb1be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize three weight matrices Wq, Wk, Wv\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad= False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad= False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c4d502d-263b-4f5c-8a3d-6791ca76ed88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n"
     ]
    }
   ],
   "source": [
    "print(W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2c411b-ea1b-4dd4-95a6-06445802dc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n"
     ]
    }
   ],
   "source": [
    "print(W_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad77b2e-65af-4616-8bf3-e8cae61be502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "print(W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35bc232-4b5c-4980-91a0-08030341e6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# We are just working for the word journey\n",
    "\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ea95995-cfb6-4262-8d98-e2d1b4759eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "queries.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "queries = inputs @ W_query\n",
    "\n",
    "print(\"key.shape:\", keys.shape)\n",
    "\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "print(\"queries.shape:\", queries.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79e801-4391-4948-9ac1-b83fe86f6859",
   "metadata": {},
   "source": [
    "- We have successfully projected the 6 input tokens from a 3D onto a 2D embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f4eaa1f-2234-48d6-94ac-68729d1dc04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# Attention score w22\n",
    "\n",
    "keys_2 = keys[1] #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75686ad6-dcb7-4670-906e-316b6f33fcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# We can generalize this computation to all attention scores via matrix multiplication\n",
    "attn_score_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_score_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d02f11-d879-4143-ae1b-a4fba720d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = queries @ keys.T # omega\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976f1a2-21ce-4132-99e0-112ee0c90f66",
   "metadata": {},
   "source": [
    "- We compute the attention weights by scaling the attention scores and using the softmax function\n",
    "\n",
    "- The difference to earlier is that we now scale the attention scores by dividing them by the square root of keu dimensions\n",
    "\n",
    "- Taking square root is mathematically the same as exponentiating by 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a5d9906-bddc-42a6-a1cb-969072bf6a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1] # keys dimension - (-1) as we are looking at the column\n",
    "attn_weights_2 = torch.softmax(attn_score_2 / d_k ** 0.5, dim=-1)\n",
    "print(attn_weights_2)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3cf1bd-5f0b-472f-b6bb-dbd3170007c7",
   "metadata": {},
   "source": [
    "Why divide by sqrt (dimensions)\n",
    "\n",
    "- For stability in learning : The softmax function is sensitive to the magnitudes of the inputs. When the inputs are large, the differences between the exponential values of each input become much more pronounced. This causes the softmax output to become \"peaky\" where the highest value receives almost all the probability mass, and the rest receive very litte.\n",
    "- In attention mechanisms, particularly in tranformers, if the dot products between query and key vectors become too large, the attention scores can become very large. This results in a very sharp softmax distribution, making the model overly confident in one particular \"key\". Such sharp distributions can make learnings unstable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abfb179e-2b08-44af-a60c-e6b1821a8518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax without scaling: tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "Softmax after scaling (tensor * 8): tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Define the tensor\n",
    "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "\n",
    "# Apply softmax without scaling \n",
    "softmax_result = torch.softmax(tensor, dim= -1)\n",
    "print(\"Softmax without scaling:\", softmax_result)\n",
    "\n",
    "# Multiply the tensor by 8 and then apply softmax \n",
    "scaled_tensor = tensor * 8\n",
    "softmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\n",
    "print(\"Softmax after scaling (tensor * 8):\", softmax_scaled_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c09111-57f2-4233-8f10-11be0d8465fd",
   "metadata": {},
   "source": [
    "- Softmax of first tensor are good, but for the second tensor they are disproportionately high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d944f814-120e-4b07-b441-f28d0ae5f5cd",
   "metadata": {},
   "source": [
    "Why sqrt ? \n",
    "- To make the variance of the dot product stable\n",
    "- The dot product of Q and K increases the variance because multiplying two random numbers increases the variance\n",
    "- The increase in variance grows with the dimension\n",
    "- Dividing by sqrt (dimension) keeps the variance close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5f4362f-cd07-4b08-aeb7-a98cbab290f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance before scaling (dim=5): 5.379663029774778\n",
      "Variance after scaling (dim=5): 1.0759326059549552\n",
      "Variance before scaling (dim=20): 21.449392491789247\n",
      "Variance after scaling (dim=20): 1.0724696245894623\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compute variance before and after scaling\n",
    "def compute_variance(dim, num_trails=1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute dot products \n",
    "    for _ in range(num_trails):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "\n",
    "        # Compute dot product\n",
    "        dot_product = np.dot (q,k)\n",
    "        dot_products.append(dot_product)\n",
    "\n",
    "        # Scale the dot product by sqrt(dim)\n",
    "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "\n",
    "    # Calculate variance of the dot products \n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "# For dimension 5\n",
    "variance_before_5, variance_after_5 = compute_variance(5)\n",
    "print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n",
    "print(f\"Variance after scaling (dim=5): {variance_after_5}\")\n",
    "\n",
    "# For dimension 20\n",
    "variance_before_20, variance_after_20 = compute_variance(20)\n",
    "print(f\"Variance before scaling (dim=20): {variance_before_20}\")\n",
    "print(f\"Variance after scaling (dim=20): {variance_after_20}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377ab9c-a0f9-4ff2-8e9f-184ae349f033",
   "metadata": {},
   "source": [
    "- If dimension = 5, then the variance is close to 5\n",
    "- After scaling the variance is always close to 1 --> that's why sqrt is used "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a79b5-c3b1-445b-aa43-bf8e55d795c1",
   "metadata": {},
   "source": [
    "- Context vector is a weighted sum over the value vectors \n",
    "- Here, the attention weights serve as a weighting factor that weighs the respective importance of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bf04f3c-56cc-434c-aa22-c7748adab944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19069037-d2e2-4ee6-8eb4-cbe54c7266f1",
   "metadata": {},
   "source": [
    "# Implementing a compact self attention python class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b266045-5864-4862-879d-a74fbaed5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out): \n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context_vec = attn_weights @ values \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e9be9-8652-4727-92d2-2bed58e9e1d4",
   "metadata": {},
   "source": [
    "- In PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a fundamental building block of PyTorch models, which provides necessary functionalities for model layer creation and management\n",
    "- The init method initializes trainable weight matrices (W_query, W_key and W_value) for queries, keys and values, each transforming the input dimension d_in to an output dimension d_out.\n",
    "- During the forward pass, using the forward method, we compute the attention scores (attn_scores) by multiplying queries and keys, normalizing these scores using softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8af52b42-a609-486e-9992-8d0c2c53efe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5156f78f-8729-4b6f-af77-bdce0be00c41",
   "metadata": {},
   "source": [
    "- We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's nn.Linear layers, which effectively perform matrix multiplication when the bias units are diabled.\n",
    "- Additionally, a significant advantage of using nn.Linear instead of manually\n",
    "implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight\n",
    "initialization scheme, contributing to more stable and effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d2245c7-db92-4b82-aa1a-493e30f6eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False): \n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context_vec = attn_weights @ values \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "836aff4d-980d-4bf9-9a3d-0efc0d3a32eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415f296-4602-4478-bb1e-dbd4fef8abe8",
   "metadata": {},
   "source": [
    "- Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they\n",
    "use different initial weights for the weight matrices since nn.Linear uses a more\n",
    "sophisticated weight initialization scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca8358-d10d-4341-bb07-9c99db1ddc7d",
   "metadata": {},
   "source": [
    "# Causal attention starts here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1c4c7f2-ad26-4a37-993b-a998534d7156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs) #A\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ key.T\n",
    "attn_weights = torch.softmax(attn_scores / key.shape[-1] ** 0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731fbd3b-68ec-4b34-9dcb-4165cb2fbcf7",
   "metadata": {},
   "source": [
    "- We can use PyTorch tril function to create a mask where the values above the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f83e8572-dbb6-4fac-809a-9f0f1fd0c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a780a-1235-4cae-9989-1a85f522e5fb",
   "metadata": {},
   "source": [
    "- Now we can multiply this mask with the attention weights to zero out the values above the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d924c5c0-2616-4212-8ee3-bef55d34e492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask_simple = attn_weights * mask_simple\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1248f34c-7576-48c1-8378-1d59c8466797",
   "metadata": {},
   "source": [
    "- Now we have to renormalize the attention weight to sum up to 1 again in each row\n",
    "- We can achieve this by dividing each element in each row by the sum in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcdf2dd8-cf7b-4a22-bd72-e2a2fff8063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = mask_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = mask_simple/ row_sums \n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b7cca-9a56-40fb-b4bb-656eb4bd766e",
   "metadata": {},
   "source": [
    "- The result is an attention weight matrix where the attention weights above the diagonal are zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169b51e-ed57-4a9f-bfb2-5709cba00719",
   "metadata": {},
   "source": [
    "- But, in the above, when we create attention weight, all the words influence each other (the words after too) -> data leakage\n",
    "- We can implement the computation of the masked attention weights more efficiently in fewer steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c0c70-2191-4c6f-aab0-6087666ee3c3",
   "metadata": {},
   "source": [
    "- The softmax function converts its inputs into a probability distribution.\n",
    "- When negative infinity values (-∞) are present in row, the softmax function treats them as zero probability. (Mathematically, this is because e -∞ approaches 0)\n",
    "- We can implement this more efficient masking \"trick\" by creating a mask with 1's above the diagonal and then replacing these 1's with negative infinity (-inf) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2be98c4c-c461-4438-ac78-7c2c991fb0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
      "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
      "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
      "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
      "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
      "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4d0d630-2d31-40da-a993-7e78d1eb74a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71910c9c-47b1-4287-9b38-05370c9833b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d483f1a5-4e30-43d6-bb19-4cb8b4e88219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now we apply the softmax function to these masked results\n",
    "\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be7d22-b48a-4785-9219-f29929af5922",
   "metadata": {},
   "source": [
    "- As we can see based on the output, the values in each row sum to 1, and no further\n",
    "normalization is necessary.\n",
    "- Masking in Transformers sets scores for future tokens to a large negative value, making their influence in the softmax calculation effectively zero. \n",
    "- The softmax function then recalculates attention weights only among the unmasked tokens. \n",
    "- This process ensures no information leakage from masked tokens, focusing the model solely on the intended data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d67d70-4b5f-4f12-89ec-756cb08db050",
   "metadata": {},
   "source": [
    "### Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2689cbb0-69fd-4d44-b5f5-b2e799695e5a",
   "metadata": {},
   "source": [
    "- In the following, we use a dropout rate of 50%, which means masking out half of the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bef9eb34-36ee-4bd6-aec9-55a126af6600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6,6) #B\n",
    "print(dropout(example))\n",
    "\n",
    "# we take a matrix of 1s and dropout 50%, so the other weights will be rescaled by 2. \n",
    "# these are probabilistic, on avg 3 neurons will be switched off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4fb781-9410-4114-b2a1-1a5a1026f3a6",
   "metadata": {},
   "source": [
    "- When applying dropout to an attention weight matrix with a rate of 50%, half of the\n",
    "elements in the matrix are randomly set to zero. \n",
    "\n",
    "- To compensate for the reduction in active\n",
    "elements, the values of the remaining elements in the matrix are scaled up by a factor of\n",
    "1/0.5 =2. \n",
    "\n",
    "- This scaling is crucial to maintain the overall balance of the attention weights,\n",
    "ensuring that the average influence of the attention mechanism remains consistent during\n",
    "both the training and inference phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e6cb90a-c74d-41f1-bbd7-9d68a1f3aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4beaf6-20e6-4164-b729-7e8d335f8175",
   "metadata": {},
   "source": [
    "- As we can see above, the resulting attention weight matrix now has additional elements zeroed out and the\n",
    "remaining ones rescaled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1cc8c-6eb5-4e59-acb8-affe69f8f714",
   "metadata": {},
   "source": [
    "# Implementing a compact causal attention class "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f627ee1-0cac-4db4-9bcb-369a741222bb",
   "metadata": {},
   "source": [
    "- Before we begin, one more thing is to ensure that the code can handle batches\n",
    "consisting of more than one input. \n",
    "\n",
    "- This will ensure that the CausalAttention class supports the batch\n",
    "outputs produced by the data loader we implemented earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2a29f-fe5c-4d19-a6fb-d5a87b01f62f",
   "metadata": {},
   "source": [
    "- For simplicity, we use 2 inputs with 6 tokens each, and each token has embedding dimension 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2495416c-34b1-4bdb-af9f-fa96c6a74b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs),dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30751e58-4da6-46ec-9e3e-be601ecc62b4",
   "metadata": {},
   "source": [
    "- This results in a 3D tensor consisting of 2 input texts with 6 tokens each, where each token\n",
    "is a 3-dimensional embedding vector.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2612df-95d8-4d19-930e-046e6b52e514",
   "metadata": {},
   "source": [
    "- Step 1: Compared to the previous SelfAttention_v1 class, we added a dropout layer.\n",
    "    \n",
    "- Step 2: The register_buffer call is also a new addition\n",
    "\n",
    "- Step 3:  We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).\n",
    "\n",
    "- Step 4: In PyTorch, operations with a trailing underscore are performed in-place, avoiding unnecessary memory\n",
    "copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58376d18-de9d-4e75-8c05-b8bef07c0d26",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal = 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1,2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1] ** 0.5, dim = -1\n",
    "        )\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0bd3459-89a4-488c-9ef7-de9522c9a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65504f79-78a5-4891-afb5-13d7173efa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d99dfd-b257-454a-8979-9ce3511eea2e",
   "metadata": {},
   "source": [
    "- The use of register_buffer inPyTorch is not strictly necessary for all use cases but offers several advantages here. \n",
    "\n",
    "- For\n",
    "instance, when we use the CausalAttention class in our LLM, buffers are automatically\n",
    "moved to the appropriate device (CPU or GPU) along with our model, which will be relevant\n",
    "when training the LLM in future chapters. \n",
    "\n",
    "- This means we don't need to manually ensure\n",
    "these tensors are on the same device as your model parameters, avoiding device mismatch\n",
    "errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfab560-baf9-4739-ad02-a5b8143c4193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
