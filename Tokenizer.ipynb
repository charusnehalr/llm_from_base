{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2393bcc-177f-456b-8156-56588ff88686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20780\n",
      "THE VERDICT\n",
      "June 1908\n",
      "I had always thought Jack Gisburn rather a cheap genius--though a\n",
      "\n",
      "good fello\n"
     ]
    }
   ],
   "source": [
    "with open(\"verdict.txt\", \"r\", encoding=\"utf-8\")as f: \n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c6e3df-5e0a-440a-9339-6f6031cb89d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1b37f6-0321-4741-8ba1-d5773a1a7c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', ' world', '.', ' This', ',', ' is a test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|/\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ef5a7f-1f63-4b58-82f1-ddec2f08da29",
   "metadata": {},
   "source": [
    "[new_item for item in iterable if condition]\n",
    "\n",
    "new_list = []\n",
    "for item in iterable:\n",
    "    if condition:\n",
    "        new_list.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f3f3b6-9a5d-45f2-ae77-20f382f230ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', ' world', '.', ' This', ',', ' is a test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de34f0c2-eed6-4d81-a148-50184bf3448b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n",
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "print(result)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dfc0f5-9055-43d1-afd0-6b9dad25053a",
   "metadata": {},
   "source": [
    "### Step 1: Tokenize the short story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b31d7c0-8d32-4165-8a00-d4cba11ac534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE', 'VERDICT', 'June', '1908', 'I', 'had', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a286297-e6df-44db-94e3-cfbb0edb69f1",
   "metadata": {},
   "source": [
    "### Step 2: Convert to token id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96238c28-0712-44e3-95aa-68ddcd1989c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f10ec-4f1e-41c6-8f98-8ecaf7fdf156",
   "metadata": {},
   "source": [
    "the below code is same as \n",
    "\n",
    "vocab = {}\n",
    "\n",
    "for integer, token in enumerate(all_words):\n",
    "    vocab[token] = integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb603cad-424c-4b46-9942-221fa780d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary\n",
    "# go through each word in all_words\n",
    "# give it a number based on it position\n",
    "# store the word as the key and the number as the value in a dictionary\n",
    "\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "# for...in... inside {} --> dictionary comprehension --> short way of writing a loop that builds a dictionary\n",
    "# for integer, token in enumerate().... is same as ..for (integer, token) in enumerate(all_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebc45b06-3d96-4e23-a58c-2ed59a7cc4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "('1908', 8)\n",
      "(':', 9)\n",
      "(';', 10)\n",
      "('?', 11)\n",
      "('A', 12)\n",
      "('AM', 13)\n",
      "('Ah', 14)\n",
      "('Among', 15)\n",
      "('And', 16)\n",
      "('Are', 17)\n",
      "('Arrt', 18)\n",
      "('As', 19)\n",
      "('At', 20)\n",
      "('Be', 21)\n",
      "('Begin', 22)\n",
      "('Burlington', 23)\n",
      "('But', 24)\n",
      "('By', 25)\n",
      "('Carlo', 26)\n",
      "('Carlo;', 27)\n",
      "('Chicago', 28)\n",
      "('Claude', 29)\n",
      "('Come', 30)\n",
      "('Croft', 31)\n",
      "('Destroyed', 32)\n",
      "('Devonshire', 33)\n",
      "('Don', 34)\n",
      "('Dubarry', 35)\n",
      "('Emperors', 36)\n",
      "('End', 37)\n",
      "('FELT', 38)\n",
      "('Florence', 39)\n",
      "('For', 40)\n",
      "('Gallery', 41)\n",
      "('Gideon', 42)\n",
      "('Gisburn', 43)\n",
      "('Gisburns', 44)\n",
      "('Grafton', 45)\n",
      "('Greek', 46)\n",
      "('Grindle', 47)\n",
      "('Grindles', 48)\n",
      "('HAD', 49)\n",
      "('HAS', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58f04fe8-8a0f-4b16-8570-e713fbaf46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1: \n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc5ad1ff-0b7c-4acd-9b35-762af6aa561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler version of the above code\n",
    "class SimplerTokenizerV1: \n",
    "    def __init__(self, vocab): \n",
    "        self.str_to_int = vocab\n",
    "        \n",
    "        # build reverse dictionary step by step\n",
    "        self.int_to_str = {}\n",
    "        for s, i in vocab.items():\n",
    "            self.int_to_str[i] = s\n",
    "\n",
    "    def encode(self, text):\n",
    "        # split text into tokens\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        # clean tokens\n",
    "        new_list = []\n",
    "        for item in preprocessed: \n",
    "            if item.strip():\n",
    "                new_list.append(item.strip())\n",
    "\n",
    "        preprocessed = new_list\n",
    "\n",
    "        # convert tokens to ids\n",
    "        ids = []\n",
    "        for s in preprocessed: \n",
    "            ids.append(self.str_to_int[s])\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # convert ids back to tokens\n",
    "        text_parts = []\n",
    "        for i in ids: \n",
    "            text_parts.append(self.int_to_str[i])\n",
    "\n",
    "        text = \"\".join(text_parts)\n",
    "\n",
    "        # remove spaces before punctuation\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15c94e3d-ba80-489d-b25e-f26fa4682b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 63, 2, 879, 1017, 627, 555, 772, 5, 1157, 621, 5, 1, 77, 7, 43, 880, 1139, 782, 822, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "            Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83042682-7456-4f39-9956-5aeb1d6c604d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee310dc2-3a6c-4142-9079-38700d1fd309",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# hello is not present in the vocabulary - so we need large dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenizer.encode(text))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m      9\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m ids = [\u001b[38;5;28mself\u001b[39m.str_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "# hello is not present in the vocabulary - so we need large dataset\n",
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f9670-c168-4697-8e1a-e7ae44d68338",
   "metadata": {},
   "source": [
    "### Adding special context tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67802bae-3f0a-49d8-90db-9227c1c1a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd154d5f-154a-479a-9094-727220444aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe232acb-aa54-464f-b837-4db8e16b8eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1158)\n",
      "('your', 1159)\n",
      "('yourself', 1160)\n",
      "('<|endoftext|>', 1161)\n",
      "('<|unk|>', 1162)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate (list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4193c730-c80a-446c-93a0-9bf9e4999d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace space bafore the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.;:?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f4b6e40-faea-4c58-bf97-2b0c343f0c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96ee0b2d-8203-437e-ae55-6e44f1565d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1162,\n",
       " 5,\n",
       " 377,\n",
       " 1157,\n",
       " 653,\n",
       " 1004,\n",
       " 11,\n",
       " 1161,\n",
       " 62,\n",
       " 1017,\n",
       " 985,\n",
       " 1013,\n",
       " 748,\n",
       " 1017,\n",
       " 1162,\n",
       " 7]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75b8454b-e897-4e73-957c-4d77ba41161b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddcf48c-d329-499e-868f-641f9bc7613b",
   "metadata": {},
   "source": [
    "Other tokens \n",
    "- BOS (beginning of sequence): Token marks the start of a text\n",
    "- EOS (end of sequence): Token positioned at the end of a text\n",
    "- PAD (padding): when training LLM with batch size larger than one, the batch might contain text of varying lengths. To ensure all the text have the same length the shorter text are extended or padded using PAD token, up to the lenght of the longest text in the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15cc7957-0341-4d9e-9dd6-7351700d4dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\rajarathinam\\anaconda3\\envs\\myenv\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rajarathinam\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rajarathinam\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rajarathinam\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajarathinam\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
      "Downloading tiktoken-0.12.0-cp313-cp313-win_amd64.whl (879 kB)\n",
      "   ---------------------------------------- 0.0/879.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 879.1/879.1 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl (277 kB)\n",
      "Installing collected packages: regex, tiktoken\n",
      "\n",
      "   ---------------------------------------- 2/2 [tiktoken]\n",
      "\n",
      "Successfully installed regex-2025.11.3 tiktoken-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2720035-6dda-4433-aacc-463161e3e718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3cb369bd-8144-480e-afd6-85fcfea570f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b29c51b6-feaf-4a61-b6cd-de29470bf6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 850, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sublit terraces\"\n",
    "    \"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f4a445e-2e35-474b-a9cc-1dc142720162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sublit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6744e7-9e90-495a-83ef-bf6df2b810ed",
   "metadata": {},
   "source": [
    "### Random example for Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "538f1ff6-a6d6-47f0-b61b-b3b1af803fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5c5f2b2-7584-4fb2-b5d8-ef75d767354f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a37e43-668d-4ac2-bc19-8459f6d12066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
